# 寒假学习复盘 (2月23日)

## 一、已完成内容
- 前馈神经网络
-卷积神经网络
-循环神经网络

## 二、核心收获
- **常用的激活函数**：ReLU函数
- **前馈神经网络**：最基本的全连接型网络，是对反向传播算法最初的运用，例子：MNIST数字图片集
- **卷积神经网络**：CNN，其本质是利用数学上的卷积运算机制来对于图像类信息进行像素信息遍历扫描（假设开始由n个通道，我们希望提取m个特征，于是我们用m个卷积核，每个卷积核n个滤波器来扫，绕后每个卷积核的结果合并，得到的新的结果就会有n个通道，其一般都由一套固定的结构组成：
卷积层——激活函数——汇聚层——全连接层；模型：LeNet-5：三轮卷积层到汇聚层的循环，最后全连接层，C3卷积层运用连接表来缩减尺寸；例子： CIFAR-10 训练任务
-**循环神经网络**： RNN，其本质是为了避免“遗忘”而在时间层面上建立先后联系的网络结构，隐藏层的活性值ht是带着记忆信息的参数；门控机制，其目的是解决长程依赖问题导致的梯度爆炸以及记忆容量不够俩问题，通过输入门遗忘门输出门来对信息进一步加工。
## 三、当前卡点
-循环神经网络还缺少实战练习
## 四、下一步计划
- Huggingface pipeline的学习，尽快确定最终的学习成果项目内容，保证最后的产出。